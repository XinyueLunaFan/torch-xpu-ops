name: E2E Nightly_OnDemand Tests

on:
  schedule:
    # GMT+8 21:00 every day
    - cron: '0 13 * * *'
  workflow_dispatch:
    inputs:
      python:
        required: false
        type: string
        default: '3.10'
        description: Specify python version
      triton:
        required: false
        type: string
        default: ''
        description: Specify triton commit, use pytorch pined commit by default
      suite:
        required: true
        type: string
        default: 'huggingface'
        description: Dynamo benchmarks test suite. huggingface,timm_models,torchbench. Delimiter is comma
      dt:
        required: true
        type: string
        default: 'float32'
        description: Data precision of the test.float32,bfloat16,float16,amp_bf16,amp_fp16. Delimiter is comma
      mode:
        required: true
        type: string
        default: 'inference'
        description: inference,training. Delimiter is comma
      scenario:
        required: true
        type: string
        default: 'accuracy'
        description: accuracy,performance. Delimiter is comma
      model:
        required: false
        type: string
        default: ''
        description: If set, will only launch this one
      ut_suite:
        required: true
        type: string
        default: 'op_example'
        description: op_example,op_extended,op_ut,torch_xpu. Delimiter is comma
      torch_xpu_ops_update:
        required: false
        type: string
        default: 'true'
        description: True means update xpu_ops when building pytorch, otherwise means not
      test_ut:
        required: true
        type: string
        default: 'true'
        description: True will running UT test for Ondemand test, false will not.


permissions: read-all

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref_name }}-${{ github.ref_type == 'branch' && github.sha }}-${{ github.event_name == 'workflow_dispatch' }}-${{ github.event_name == 'schedule' }}-${{ inputs.suite }}-${{ inputs.dt }}-${{ inputs.mode }}-${{ inputs.scenario }}-${{ inputs.triton }}-${{ inputs.model }}
  cancel-in-progress: true

jobs:
  Inductor-XPU-E2E-Nightly-Tests:
    runs-on: pvc_e2e
    # Don't run on forked repos
    if: github.repository_owner == 'intel'
    timeout-minutes: 900
    outputs:
      TORCH_BRANCH_ID: ${{ steps.pinned.outputs.TORCH_BRANCH_ID }}
      TORCH_COMMIT_ID: ${{ steps.pinned.outputs.TORCH_COMMIT_ID }}
      DRIVER_VERSION: ${{ steps.pinned.outputs.DRIVER_VERSION }}
      BUNDLE_VERSION: ${{ steps.pinned.outputs.BUNDLE_VERSION }}
      OS_PRETTY_NAME: ${{ steps.pinned.outputs.OS_PRETTY_NAME }}
      GCC_VERSION: ${{ steps.pinned.outputs.GCC_VERSION }}
    steps:
      - name: Checkout torch-xpu-ops
        uses: actions/checkout@v4
      - name: Prepare Conda ENV
        run: |
          which conda && conda clean -ay
          conda remove --all -y -n e2e_ci || rm -rf $(dirname ${CONDA_EXE})/../envs/e2e_ci
          conda create -n e2e_ci python=${{ inputs.python }} cmake ninja -y
          source activate e2e_ci
          conda install -c intel mkl-static mkl-include -y
          pip install pandas scipy tqdm
      - name: Prepare Stock Pytorch
        run: |
          pwd
          cd ../ && rm -rf pytorch
          source activate e2e_ci
          git clone -b main https://github.com/pytorch/pytorch pytorch
          cd pytorch
          # apply PRs for stock pytorch
          pip install requests
          python ../torch-xpu-ops/.github/scripts/apply_torch_pr.py
          git status && git show -s
          git submodule sync && git submodule update --init --recursive
          if [ -z ${{ inputs.torch_xpu_ops_update }} ]; then
            rm -rf third_party/torch-xpu-ops && cp -r ../torch-xpu-ops third_party/
          else
            if [[ ${{ inputs.torch_xpu_ops_update }} == 'true' ]]; then
              rm -rf third_party/torch-xpu-ops && cp -r ../torch-xpu-ops third_party/
            else
              echo "Not update torch-xpu-ops"
            fi
          fi
          # Workaround for torch-xpu-ops ci test
          sed -i "s/checkout --quiet \${TORCH_XPU_OPS_COMMIT}/log -n 1/g" caffe2/CMakeLists.txt
      - name: Identify pinned versions
        id: pinned
        run: |
          cd ../pytorch
          if [ -z ${{ inputs.triton }} ]; then
            echo "TRITON_COMMIT_ID=$(<.ci/docker/ci_commit_pins/triton-xpu.txt)" >> "${GITHUB_ENV}"
          else
            echo "TRITON_COMMIT_ID=${{ inputs.triton }}" >> "${GITHUB_ENV}"
          fi
          echo "TORCH_BRANCH_ID=$(git rev-parse --abbrev-ref HEAD)" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          echo "TORCH_COMMIT_ID=$(git rev-parse HEAD)" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          echo "TORCHBENCH_COMMIT_ID=$(<third_party/torch-xpu-ops/.github/ci_commit_pins/torchbench.txt)" >> "${GITHUB_ENV}"
          echo "TORCHVISION_COMMIT_ID=$(<.github/ci_commit_pins/vision.txt)" >> "${GITHUB_ENV}"
          echo "TORCHTEXT_COMMIT_ID=$(<.github/ci_commit_pins/text.txt)" >> "${GITHUB_ENV}"
          echo "TORCHAUDIO_COMMIT_ID=$(<.github/ci_commit_pins/audio.txt)" >> "${GITHUB_ENV}"
          echo "TRANSFORMERS_VERSION=$(<.ci/docker/ci_commit_pins/huggingface.txt)" >> "${GITHUB_ENV}"
          echo "TIMM_COMMIT_ID=$(<.ci/docker/ci_commit_pins/timm.txt)" >> "${GITHUB_ENV}"
          echo "MODEL_ONLY_NAME=${{ inputs.model }}" >> "${GITHUB_ENV}"
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          echo "DRIVER_VERSION=$(dkms status 2>&1 |grep 'intel-i915-dkms' |sed 's/.*\///;s/,.*//')" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          echo "BUNDLE_VERSION=$(dpcpp --version 2>&1 |grep 'DPC++/C++' |sed 's/.*(//;s/).*//')" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          . /etc/os-release
          echo "OS_PRETTY_NAME=${PRETTY_NAME}" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          echo "GCC_VERSION=$(gcc -dumpversion)" |tee -a "${GITHUB_OUTPUT}" >> "${GITHUB_ENV}"
          echo ${GITHUB_ENV}
      - name: Triton Installation
        run: |
          source activate e2e_ci
          cd ../pytorch
          TRITON_REPO="https://github.com/intel/intel-xpu-backend-for-triton"
          echo ${TRITON_REPO}@${TRITON_COMMIT_ID}
          pip install --force-reinstall "git+${TRITON_REPO}@${TRITON_COMMIT_ID}#subdirectory=python"
      - name: Build Pytorch XPU
        run: |
          source activate e2e_ci
          cd ../pytorch
          pip install -r requirements.txt
          export USE_XPU=1
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
          python setup.py bdist_wheel
          pip install --force-reinstall dist/*.whl
      - name: Show GITHUB_ENV
        run: |
          echo "$GITHUB_ENV"
          rm -rf ../pytorch/inductor_log
      - name: Nightly Huggingface FP32 Inference Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          env_prepare: true
          dt: float32
          mode: inference
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Huggingface BF16 Inference Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          dt: bfloat16
          mode: inference
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Huggingface FP16 Inference Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          dt: float16
          mode: inference
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Huggingface FP32 Training Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          env_prepare: true
          dt: float32
          mode: training
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Huggingface BF16 Training Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          dt: bfloat16
          mode: training
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Huggingface FP16 Training Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: huggingface
          dt: float16
          mode: training
          scenario: accuracy
          expected_pass_num: 46
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Torchbench BF16 Training Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: torchbench
          dt: bfloat16
          mode: training
          scenario: accuracy
          expected_pass_num: 39
          env_prepare: true
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Nightly Timm_models FP16 Training Accuracy Test
        if: ${{ !inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: timm_models
          dt: float16
          mode: training
          scenario: accuracy
          expected_pass_num: 22
          env_prepare: true
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: OnDemand Test (${{ inputs.suite }} ${{ inputs.dt }} ${{ inputs.mode }} ${{ inputs.scenario }})
        if: ${{ inputs.suite }}
        uses: ./.github/actions/inductor-xpu-e2e-test
        with:
          suite: ${{ inputs.suite }}
          env_prepare: true
          dt: ${{ inputs.dt }}
          mode: ${{ inputs.mode }}
          scenario: ${{ inputs.scenario }}
          hf_token: ${{ secrets.HUGGING_FACE_HUB_TOKEN }}
      - name: Summarize archieve files
        run: |
          rm -rf ${{ github.workspace }}/upload_files
          cp -r ${{ github.workspace }}/../pytorch/inductor_log ${{ github.workspace }}/upload_files
      - name: Upload Inductor XPU E2E Data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: Inductor-XPU-E2E-Data-${{ github.event.pull_request.number || github.sha }}
          path: ${{ github.workspace }}/upload_files

  Inductor-XPU-UT-Nightly-Tests:
    if: inputs.test_ut == 'true'
    runs-on: linux.idc.xpu
    timeout-minutes: 900
    steps:
      - name: Checkout torch-xpu-ops
        uses: actions/checkout@v4
      - name: Prepare Stock Pytorch
        run: |
          pwd
          cd ../ && rm -rf pytorch
          git clone -b main https://github.com/pytorch/pytorch
          cd pytorch && git log -n 1 && git submodule sync && git submodule update --init --recursive
          if [ -z ${{ inputs.torch_xpu_ops_update }} ]; then
            rm -rf third_party/torch-xpu-ops && cp -r ../torch-xpu-ops third_party/
          else
            if [[ ${{ inputs.torch_xpu_ops_update }} == 'true' ]]; then
              rm -rf third_party/torch-xpu-ops && cp -r ../torch-xpu-ops third_party/
            else
              echo "Not update torch-xpu-ops"
            fi
          fi
          # Workaround for torch-xpu-ops ci test
          sed -i "s/checkout --quiet \${TORCH_XPU_OPS_COMMIT}/log -n 1/g" caffe2/CMakeLists.txt
      - name: Build Pytorch XPU
        run: |
          which conda && conda clean -ay
          conda remove --all -y -n xpu_op_${ZE_AFFINITY_MASK} || \
                rm -rf $(dirname ${CONDA_EXE})/../envs/xpu_op_${ZE_AFFINITY_MASK}
          conda create -n xpu_op_${ZE_AFFINITY_MASK} python=3.10 cmake ninja -y
          source activate xpu_op_${ZE_AFFINITY_MASK}
          conda install -c intel mkl-static mkl-include -y
          cd ../pytorch
          pip install -r requirements.txt
          export USE_XPU=1
          source /opt/intel/oneapi/compiler/latest/env/vars.sh
          export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}
          python setup.py bdist_wheel
          pip install --force-reinstall dist/*.whl
          git clone https://github.com/pytorch/vision && cd vision && python setup.py install && cd ..
          pip install -r .ci/docker/requirements-ci.txt
      - name: Nightly XPU OP Examples UT Test
        if: ${{ !inputs.ut_suite }}
        uses: ./.github/actions/inductor-xpu-ut-test
        with:
          ut_suite: op_example
          expected_fail_num: 0
      - name: Nightly XPU OP Extended UT Test
        if: ${{ !inputs.ut_suite }}
        uses: ./.github/actions/inductor-xpu-ut-test
        with:
          ut_suite: op_finegrain
          expected_fail_num: 0
      - name: Nightly XPU OP UT Test
        if: ${{ !inputs.ut_suite }}
        uses: ./.github/actions/inductor-xpu-ut-test
        with: 
          ut_suite: op_ut
          expected_fail_num: 0
      - name: Nightly Torch XPU UT Test
        if: ${{ !inputs.ut_suite }}
        uses: ./.github/actions/inductor-xpu-ut-test
        with:
          ut_suite: torch_xpu
          expected_fail_num: 0
      - name: OnDemand UT Test (${{ inputs.ut_suite }})
        if: ${{ inputs.ut_suite }}
        uses: ./.github/actions/inductor-xpu-ut-test
        with:
          ut_suite: ${{ inputs.ut_suite }}
    
  Tests-Failure-And-Report:
    if: always()
    runs-on: pvc_e2e
    permissions:
      issues: write
    env:
      GH_TOKEN: ${{ github.token }}
    needs: Inductor-XPU-E2E-Nightly-Tests
    steps:
      - name: Report github issue for XPU OPS nightly
        if: github.repository_owner == 'intel'
        run: |
          set -xe
          # Test status
          if [ "${{ needs.Inductor-XPU-E2E-Nightly-Tests.result }}" == "success" ];then
            test_status=Success
          elif [ "${{ needs.Inductor-XPU-E2E-Nightly-Tests.result }}" == "failure" ];then
            test_status=Failure
            cc_comment="CC ${{ secrets.NIGHTLY_EMAIL_LIST }}"
          else
            test_status=None
            exit 0
          fi
          build_url="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          repo="${{ github.repository }}"
          test_xpu="${GITHUB_WORKFLOW_SHA} on ${GITHUB_REF_NAME}"
          test_torch="${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.TORCH_COMMIT_ID }} on ${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.TORCH_BRANCH_ID }}"
          test_driver="${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.DRIVER_VERSION }}"
          test_bundle="${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.BUNDLE_VERSION }}"
          test_os="${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.OS_PRETTY_NAME }}"
          test_gcc="${{ needs.Inductor-XPU-E2E-Nightly-Tests.outputs.GCC_VERSION }}"

          # On-demand
          if [ "${GITHUB_EVENT_NAME}" == "workflow_dispatch" ];then
            cc_comment="CC @${GITHUB_TRIGGERING_ACTOR}"
            test_scope="Inputs|suite=${{ inputs.suite }}; dt=${{ inputs.dt }}; mode=${{ inputs.mode }}; scenario=${{ inputs.scenario }};"
            if [ "${{ inputs.triton }}" != "" ];then
              test_scope+=" triton=${{ inputs.triton }};"
            fi
            if [ "${{ inputs.model }}" != "" ];then
              test_scope+=" model=${{ inputs.model }};"
            fi
            gh --repo $repo issue comment 426 --body "$cc_comment
                  $(date +'%F') On-demand Test Status: **${test_status}**
                  See: $build_url
                  Torch-xpu-ops | PyTorch
                  --- | ---
                  $test_xpu | $test_torch

                  Device | OS | GCC | Python | Driver | Bundle
                  --- | --- | --- | --- | --- | ---
                  $RUNNER_NAME | $test_os | $test_gcc | ${{ inputs.python }} | $test_driver| $test_bundle

                  Inputs | $test_scope
                  --- | ---
                  "
          # Nightly
          else
            gh --repo $repo issue comment 432 --body "$cc_comment
                  $(date +'%F') Nightly Test Status: **${test_status}**
                  See: $build_url
                  Torch-xpu-ops | PyTorch
                  --- | ---
                  $test_xpu | $test_torch

                  Device | OS | GCC | Python | Driver | Bundle
                  --- | --- | --- | --- | --- | ---
                  $RUNNER_NAME | $test_os | $test_gcc | ${{ inputs.python }} | $test_driver| $test_bundle
                  "
          fi
